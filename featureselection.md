
# Feature Selection in Machine Learning

When you train a machine learning model, not all features in your dataset are equally useful. Some may carry strong predictive signals, while others may introduce noise, redundancy, or even misleading patterns. **Feature selection** is the process of identifying and keeping the most relevant features for your model.

By carefully selecting features, you can:

* Simplify the model and make it faster to train.
* Reduce overfitting by removing irrelevant or noisy data.
* Improve interpretability by focusing on truly important inputs.
* Enhance generalization and predictive performance.

There are **four main types of feature selection techniques**.

---

## 1. Four Main Categories of Feature Selection

Feature selection methods can broadly be divided into four types, each based on when and how the selection process happens relative to the model training.

### 1.1 Filter Methods

These are **statistical, model-agnostic techniques** that assess the relationship between each feature and the target variable independently. They do not involve any actual machine learning model during selection.

**Key Idea:** Rank features using statistical scores (like correlation, Chi-square, ANOVA F-value) and select the top ones.
**Advantages:** Simple, fast.
**Limitations:** Cannot capture feature interactionsâ€Šâ€”â€Ševaluates features individually.

### 1.2 Wrapper Methods

Wrapper methods evaluate subsets of features by actually training and testing a model multiple times.

**Key Idea:** Treat feature selection as a search problem. Try different combinations of features, train a model, and choose the subset that gives the best performance.
**Examples:** Forward Selection, Backward Elimination, and Recursive Feature Elimination (RFE).
**Advantages:** Accounts for feature interactions and directly optimizes for model accuracy.
**Limitations:** Computationally expensive and prone to overfitting on small datasets.

### 1.3 Embedded Methods

These methods perform feature selection as part of the model training process itself. The model learns which features matter by applying penalties or by evaluating feature importance scores.

**Examples:** LASSO (L1 regularization), Elastic Net, and Tree-based models (like Random Forest, XGBoost).
**Advantages:** Computationally efficient and integrated within the model.
**Limitations:** Model-dependent (different algorithms produce different importance scores).

### 1.4 Feature Extraction Methods

Technically not "selection" but often used together. These methods transform the feature space into a smaller set of new features that capture most of the information.

**Examples:** PCA (Principal Component Analysis), LDA (Linear Discriminant Analysis), and t-SNE.
**Advantages:** Reduces dimensionality effectively and captures important details.
**Limitations:** New features are combinations of original onesâ€Šâ€”â€Šlosing direct interpretability.

---

## 2. Filter Methods: 

Filter methods are the most straightforward and widely used approach, especially when dealing with a large number of features. They work by ranking features based on their relevance to the target variable using a statistical metricâ€Šâ€”â€Šand keeping the top-ranked ones. These techniques are independent of the choice of model, which makes them extremely versatile.

Let's look at the most common filter methods in detail.

---

### 2.1 Variance Threshold

#### ğŸ”¹ Concept:

If a feature has the same value (or nearly the same) for most samples, it does not help the model distinguish between different classes or predict outcomes. Such low-variance features contribute little to learning.

#### ğŸ”¹ Formula:

```math
Var(X) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
```

If variance is less than a certain threshold, we can drop the column.

#### ğŸ”¹ Example:

Suppose a column `is_female` has 98% of entries as 0 and only 2% as 1. This feature has very low variance and is unlikely to help the model.

#### ğŸ”¹ Code Example:

```python
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_new = selector.fit_transform(X)
```

#### ğŸ”¹ Pros:

* Simple and computationally cheap.
* Good preprocessing step before other techniques.

#### ğŸ”¹ Cons:

* Doesn't consider relation to the target variable.
* May remove useful binary features with rare but important values.

---

### 2.2 Correlation Coefficient

#### ğŸ”¹ Concept:

Correlation shows how two numerical variables move together. If both increase or decrease in a consistent pattern, they are **positively correlated**; if one increases while the other decreases, they are **negatively correlated**. In feature selection, we use correlation to detect **redundancy**â€Šâ€”â€Šif two features are highly correlated, one can be removed without much loss of information.

There are two main uses:

1. **Feature vs Target:** Find which features strongly affect the target.
2. **Feature vs Feature:** Identify redundant features that carry the same information.

#### ğŸ”¹ Intuition:

Imagine plotting two features on a scatter plot:

* If all points fall roughly on a straight line (increasing together) â†’ high positive correlation.
* If one increases while the other decreases â†’ high negative correlation.
* If points are scattered randomly â†’ low or no correlation.

#### ğŸ”¹ Mathematical Explanation:

The **Pearson correlation coefficient** is a standardized measure of how two variables move together:

```math
r = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
```

where:

* \(  Cov(X, Y) \) is the covariance between X and Y, showing how they vary together.
* \(  \sigma_X, \sigma_Y \) are the standard deviations of X and Y, which scale the result between âˆ’1 and +1.

Covariance itself is defined as:

```math
Cov(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
```

Thus, the correlation formula tells us that:

```math
r = +1 \text{ â†’ perfect positive linear relation} \\
r = -1 \text{ â†’ perfect negative linear relation} \\
r \approx 0 \text{ â†’ no linear relationship}
```

#### ğŸ”¹ Example:

If `age` and `years_of_experience` are highly correlated (say, ( r = 0.95 )), they both increase togetherâ€Šâ€”â€Šyou can drop one to reduce redundancy.

#### ğŸ”¹ Python Example:

```python
import pandas as pd
import numpy as np  # Needed for numpy functions like triu and ones

# Calculate the absolute correlation matrix of the dataframe 'df'
# This matrix shows pairwise correlation values between each pair of features, with values from 0 to 1
corr_matrix = df.corr().abs()

# Create an upper triangle matrix of the correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Identify columns with correlation greater than 0.9 with any other column
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]

# Drop the highly correlated columns from the dataframe to reduce redundancy
df_reduced = df.drop(to_drop, axis=1)
```

#### ğŸ”¹ Pros:

* Easy to compute and visualize using a heatmap.
* Helps remove redundant information.

#### ğŸ”¹ Cons:

* Only measures linear relationships.


### 2.3 Chi-Square (Ï‡Â²) Test 

#### ğŸ”¹ What the Ï‡Â² Test Really Checks

The **Chi-Square (Ï‡Â²) Test of Independence** is one of the simplest yet most powerful statistical tools used to determine whether **two categorical variables** are **related** or **independent** of each other.

Think of it as asking:

> â€œDoes one variable tell me *anything* about the other, or are they completely unrelated?â€

In essence, the Ï‡Â² test compares what you **actually observe** in your data (the *observed frequencies*) with what you **would expect** to see if there were *no relationship* between the variables (the *expected frequencies*).

---

#### ğŸ§© Hypotheses â€” The Foundation of the Test

* **Null hypothesis (Hâ‚€):** The variables are *independent* â€” knowing one gives no information about the other.
* **Alternative hypothesis (Hâ‚):** The variables are *dependent* â€” knowing one helps predict or explain the other.

If the difference between **observed** and **expected** counts is large enough, we have evidence to reject Hâ‚€ and conclude there **is** a relationship between the two variables.

---

### ğŸ”¹ Intuition â€” A Visual Thought Experiment

Imagine you manage a store selling a new product. You record how many **males** and **females** buy or donâ€™t buy it:

| Gender    | Buy (Yes) | No     | Total   |
| --------- | --------- | ------ | ------- |
| Male      | 40        | 60     | 100     |
| Female    | 80        | 20     | 100     |
| **Total** | **120**   | **80** | **200** |

If gender and buying behavior were *independent*, then both groups (males and females) should have **roughly the same buying rate** â€” that is, the overall proportion of buyers (120 out of 200 = 60%) should hold true for both genders.

So under independence, weâ€™d expect about:

* 60% of males â†’ 60 buyers,
* 60% of females â†’ 60 buyers.

But in reality, we observe **40 male buyers** and **80 female buyers** â€” quite different from what independence predicts. Which tells us that they are not independent and there is some relationship between gender and buying pattern.

---

### ğŸ§  The Key Idea

The **Chi-Square test** quantifies *how far* the observed data deviate from the expected pattern if the variables were independent.

If these deviations are small, the data likely follow the independence assumption. If theyâ€™re large, it suggests an association â€” something systematic is going on.

Mathematically, this â€œdistanceâ€ between observed and expected counts is captured by:

```math
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
```

where:

* \( O_i \): Observed frequency in each cell  
* \( E_i \): Expected frequency (calculated as below)


```math
E_{ij} = \frac{(\text{Row Total})_i \times (\text{Column Total})_j}{\text{Grand Total}}
```

If the computed Ï‡Â² statistic is large (beyond a threshold determined by the **degrees of freedom** and **significance level**), we reject Hâ‚€ and conclude that the two variables are **statistically dependent**.

---

### ğŸ”¹ Step 1 â€” How Expected Frequencies Are Computed

When variables are independent, probabilities **factorize**:

```math
P(\text{Gender}=i, \text{Purchase}=j) = P(\text{Gender}=i) \times P(\text{Purchase}=j)
```

Now, express each probability in terms of counts:

* \( P(\text{Gender}=i) = \frac{R_i}{N} \) â€” because \( R_i \) individuals belong to gender *i* out of *N* total.  
* \( P(\text{Purchase}=j) = \frac{C_j}{N} \) â€” because \( C_j \) individuals made purchase decision *j* out of *N*.


Then,

```math
P(\text{Gender}=i, \text{Purchase}=j) = (R_i/N) \times (C_j/N) = R_iC_j / N^2
```

Multiplying both sides by total number of observations **N** gives expected counts:

```math
E_{ij} = N \times P(\text{Gender}=i) \times P(\text{Purchase}=j)
```

But since probabilities can be replaced by proportions (row and column totals over N):

```math
E_{ij} = \frac{R_i \times C_j}{N}
```

#### ğŸ’¡ Why multiply by N?

Multiplying by **N** converts a **probability** (which is a proportion of 1) into an **expected count** on the same scale as your dataset. Probabilities always sum to 1, while counts sum to N. So:

```math
E_{ij} = N \times P(\text{Gender}=i, \text{Purchase}=j)
```
For example, if you expect 30% of all customers (0.3 probability) to be Male & Yes, and N=200 total, youâ€™d expect 0.3Ã—200 = **60** such cases.

Where:

* \( R_i\): Row total (feature category)
* \( C_j\): Column total (target class)
* \( N\): Grand total

For the example:

```math
E_{Male,Yes} = \frac{100 \times 120}{200} = 60
```

Thatâ€™s what youâ€™d *expect* if gender and purchase were unrelated.

---

### ğŸ”¹ Step 2 â€” Quantifying the â€œSurpriseâ€ (Observed vs Expected)

If the observed counts (O) differ a lot from the expected counts (E), thatâ€™s evidence of association.

We measure this difference using the **Chi-Square statistic:**

```math
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
```

Each term shows how far a cellâ€™s observed count deviates from expectation â€” *standardized* by its expected value. A large Ï‡Â² â†’ stronger deviation â†’ stronger association.

---

### ğŸ”¹ Step 3 â€” Understanding the Ï‡Â² Distribution and p-Value (Beginner Friendly)

After finding the **Chi-Square (Ï‡Â²) statistic**, we need to decide whether the difference between what we **observed** and what we **expected** could have happened by chance. For that, we use two ideas â€” the **Ï‡Â² distribution** and the **p-value**.

---

#### ğŸ“ˆ What is the Chi-Square (Ï‡Â²) Distribution?

The **Chi-Square distribution** is a type of probability curve that helps us understand how likely or unlikely a certain Ï‡Â² value is. It tells us what kinds of differences we might see **just by chance** (really no relationship between our variables) and which values are real effect.

Key ideas:

* The curve always starts at 0 and goes to the right (Ï‡Â² values are never negative).
* As you move to the right, the probability becomes smaller â€” large Ï‡Â² values are rare and usually mean something unusual is happening.

The shape of this curve depends on the **degrees of freedom (df)**:

* If you have fewer categories, the curve is steep and skewed.
* If you have more categories, it becomes smoother and looks more like a bell curve.

In short: The Ï‡Â² distribution shows how â€œnormalâ€ or â€œunusualâ€ our observed difference is under the assumption that the variables are not related.

---

#### ğŸ§© Understanding Degrees of Freedom (df) and How It Shapes the Chi-Square Distribution

 The shape of **Chi-Square distribution** distribution depends entirely on something called the **degrees of freedom (df)**.

![alt text](image-1.png)
#### ğŸ”¹ What Is Degrees of Freedom (df)?

The **degrees of freedom** represent **how many independent values in a dataset can vary freely** before the totals become fixed. In a Chi-Square test, it tells us **how many independent comparisons** we can make.

The formula is:

$$
\text{df} = (r - 1)(c - 1)
$$

where:

* *r* = number of rows (categories of one variable)
* *c* = number of columns (categories of the other variable)

Each â€œâˆ’1â€ appears because once you know the totals for rows and columns, one less cell can vary freely.


#### ğŸ”¹ Table Example: How df Works

Letâ€™s look at a 2Ã—2 table:

| Gender    | Buy (Yes) | Buy (No) | Total   |
| --------- | --------- | -------- | ------- |
| Male      | ?         | ?        | 100     |
| Female    | ?         | ?        | 100     |
| **Total** | **120**   | **80**   | **200** |

If you fill in one of the inner boxes (say, number of males who bought), all other values are automatically determined by the totals. So only **one number can vary freely**, meaning **df = 1** for this table.

For larger tables (e.g., 3Ã—3 or 4Ã—2), more cells can vary independently â€” leading to higher df values.


#### ğŸ”¹ Why the Shape Changes with Degrees of Freedom

The **shape of the Chi-Square distribution depends on df** because the statistic is created by adding up multiple squared differences.

Each degree of freedom adds one more squared term to the total. More df means more information and more independent differences being summed together.

Letâ€™s see what that means with examples.


#### ğŸ”¸ Case 1: df = 1 â€” One Comparison (Single Source of Variation)

When df = 1, thereâ€™s only one independent difference between observed and expected values.

Example: Toss a coin 100 times.

* Expected: 50 heads, 50 tails.
* Observed: Usually something like 48â€“52 happens, rarely 60â€“40, very rarely 80â€“20.

Because small differences (like 48â€“52) are far more common than large ones (like 80â€“20), the Ï‡Â² value is usually small. But occasionally, a big deviation occurs.

ğŸ‘‰ This makes the Ï‡Â² curve **tall near zero** and **skewed to the right** â€” small differences are common, but big ones stretch the right tail.


#### ğŸ”¸ Case 2: df = 3 or 4 â€” Few Independent Comparisons

Now imagine comparing multiple groups, like age categories (teenagers, adults, seniors) versus buying behavior. Each group adds a separate difference between observed and expected counts.

When we add a few such squared differences together, the overall variation spreads out â€” it can take on more possible values, but extreme totals are still uncommon.

ğŸ‘‰ The curve becomes **wider and smoother**, less skewed than before.


#### ğŸ”¸ Case 3: df = 10 or More â€” Many Comparisons (Many Independent Differences)

When df becomes large, weâ€™re summing many small, random squared differences. Some of these are slightly above expectation, some below, and they start to **balance out**. The total variation becomes more predictable.

This is similar to flipping many coins â€” a single coin can give extreme results (0 or 1), but if you flip 50 coins, the average number of heads will almost always be close to 25.

ğŸ‘‰ As df increases, the curve becomes **flatter, smoother, and more symmetric**, eventually approaching a **bell-shaped (normal) distribution**.


#### ğŸ”¹ Why This Behavior Is Logical

* Each degree of freedom represents one independent random difference.
* When we have only one, extreme differences can cause big jumps â†’ skewed shape.
* As we add more, random highs and lows cancel each other out â†’ smoother shape.
* With many df, the distribution stabilizes and resembles a normal curve due to the **Central Limit Theorem** â€” the sum of many independent random values tends toward a normal distribution.


---

#### ğŸ¯ What is the p-Value?

The **p-value** tells us how likely it is to get a Ï‡Â² value as large as (or larger than) what we found **if there is actually no relationship** between the variables.

Mathematically:


```math
p = P(\chi^2_{df} \ge \chi^2_{observed})
```

This means we look at the part of the curve **to the right** of our observed Ï‡Â² value â€” that area represents the chance that such a big difference could happen randomly.

How to interpret it:

* **p < 0.05** â†’ Unlikely to be random â†’ The variables are probably **related** (reject independence).
* **p â‰¥ 0.05** â†’ Could easily be random â†’ The variables are probably **independent**.


#### ğŸ’¡ Simple Intuition

Think of the p-value as a measure of **surprise**:

* A **small p-value** means youâ€™d be very surprised to see such a big difference if the variables werenâ€™t related â†’ so you suspect a real connection.
* A **large p-value** means what you observed is not surprising at all â†’ it could easily happen by chance.

So, the Ï‡Â² distribution shows all possible random outcomes, and the p-value tells us **where our result lies** in that world of chance. Together, they help us decide whether our data show a **real relationship** or just **random noise**.


### ğŸ”¹ Step 4 â€” Example Walkthrough

| Gender    | Yes     | No     | Total   |
| --------- | ------- | ------ | ------- |
| Male      | 40      | 60     | 100     |
| Female    | 80      | 20     | 100     |
| **Total** | **120** | **80** | **200** |

**Expected frequencies:**

```math
E_{Male,Yes} = \frac{100\times120}{200}=60,\quad
E_{Male,No} = 40,\quad
E_{Female,Yes}=60,\quad
E_{Female,No}=40
```

**Compute Ï‡Â² contributions:**

```math
(Male,Yes): (40-60)^2/60 = 6.67\\
(Male,No): (60-40)^2/40 = 10.00\\
(Female,Yes): (80-60)^2/60 = 6.67\\
(Female,No): (20-40)^2/40 = 10.00
```

```math
\Rightarrow \chi^2 = 33.34
```

**Degrees of freedom:**

```math
df = (2-1)(2-1) = 1
```

**Critical value:** The point on the Chi-Square distribution where the area to its right equals the chosen p-value (significance level); results beyond this point are unlikely due to chance and lead to rejecting the null hypothesis.

```math
\chi^2_{0.05,1} = 3.841
```

Since 33.34 > 3.841 â†’ **Reject independence.** Gender and purchase behavior are statistically associated. Intuitively, our Ï‡Â² value (33.34) lies far in the tail of the Chi-Square curve â€” such a large value would almost never occur just by random variation, so the difference is real, not due to chance.



---


### ğŸ”¹ Step 5 â€” Coding It

**(A) Scikit-learn (no p-value, fast feature selection)**

```python
# Import necessary libraries
from sklearn.feature_selection import SelectKBest, chi2

# Initialize SelectKBest with chi2 function
# chi2 measures dependence between each feature and the target variable
# k=10 selects the top 10 features based on their chi-square score
selector = SelectKBest(score_func=chi2, k=10)

# Fit and transform the feature matrix X and target y
X_new = selector.fit_transform(X, y)

# Display chi-square scores for each feature
print(selector.scores_)
```

---

**(B) SciPy (with p-value)**

```python
# Import required libraries
import pandas as pd
from scipy.stats import chi2_contingency

# Create a contingency table (cross-tabulation) between two categorical variables
# Example: Gender vs Purchased
contingency = pd.crosstab(df['Gender'], df['Purchased'])

# Perform the Chi-Square Test of Independence
# chi2 = Chi-square statistic
# p = p-value (probability that the observed difference is due to chance)
# dof = degrees of freedom
# expected = expected frequencies assuming independence
chi2, p, dof, expected = chi2_contingency(contingency)

# Print test results
print(f"Chi2 = {chi2:.2f}, p-value = {p:.4f}, dof = {dof}")

# Interpretation:
# If p-value < 0.05 â†’ Variables are dependent (feature is important)
# If p-value > 0.05 â†’ Variables are independent (feature can be discarded)
```

---

**In essence:**

> The Chi-Square Test checks how much a featureâ€™s observed distribution deviates from what would be expected if it were independent of the target. If the deviation is large, the feature is informative and should be kept; if itâ€™s small, the feature is independent of the target and can be discarded.

### 2.4 ANOVA F-Test 

#### ğŸ”¹ What It Does:

The **ANOVA (Analysis of Variance) F-Test** checks if a **numerical feature** helps in **separating categories** of the target variable.

In other words â€” it answers:

> â€œDo the average feature values differ a lot across different target classes?â€

If yes â†’ the feature is useful.
If not â†’ the feature doesnâ€™t really help in distinguishing classes.

---

#### ğŸ”¹ When to Use:

Use ANOVA F-Test when:

* **Feature (X)** = continuous or numerical (e.g., height, age, income)
* **Target (y)** = categorical (e.g., Pass/Fail, Disease/No Disease)

Itâ€™s most common in **classification problems**.

---

#### ğŸ”¹ Intuitive Idea:

Imagine you have students grouped by exam results:

| Group | Average Study Hours |
| ----- | ------------------- |
| Pass  | 8.5                 |
| Fail  | 3.2                 |

If students who pass study much more on average than those who fail â€” and the variation within each group is small â€” this feature (study hours) is clearly important.

ANOVA captures this by comparing:

* **Between-group variance** â†’ how far group averages are from the overall average.
* **Within-group variance** â†’ how spread out values are inside each group.

If between-group difference is much larger â†’ F-value is **high**, meaning the feature helps separate the target classes.

---


#### ğŸ”¹ The Formula (Simplified):

```math
F = \frac{\text{Variance Between Groups}}{\text{Variance Within Groups}}
```

A **high F-value** means the group means differ more than random variation would allow â€” i.e., the feature matters.

---


#### ğŸ”¹ Expanded Mathematical Form:

Let:

* \( X_{ij} \): the j-th observation in the i-th group  
* \( \bar{X_i} \): mean of group i  
* \( \bar{X} \): overall mean (mean of all samples)  
* \( n_i \): number of samples in group i  
* \( k \): number of groups  
* \( N \): total number of samples  


Then the **Sum of Squares Between Groups (SSB)** and **Sum of Squares Within Groups (SSW)** are:

```math
SS_{between} = \sum_{i=1}^{k} n_i (\bar{X_i} - \bar{X})^2
```

```math
SS_{within} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (X_{ij} - \bar{X_i})^2
```

Now, compute the **Mean Squares** (divide by degrees of freedom):

```math
MS_{between} = \frac{SS_{between}}{k - 1}, \quad MS_{within} = \frac{SS_{within}}{N - k}
```

Finally, the **F-statistic** is:

```math
F = \frac{MS_{between}}{MS_{within}} = \frac{SS_{between} / (k - 1)}{SS_{within} / (N - k)}
```

If ( F ) is large â†’ differences between group means are large compared to random variation â†’ the feature matters.

---

#### ğŸ”¹ What is the F-Distribution?

The F-distribution is a probability curve that shows how likely different F-values are when comparing two variances (like between-group vs within-group). It always starts at 0 and extends to the right, meaning F-values canâ€™t be negative. Small F-values are common when group differences are random, while large F-values are rare and usually mean real differences exist. The shape of the curve depends on the two degrees of freedom â€” one for between-groups (kâˆ’1) and one for within-groups (Nâˆ’k). Itâ€™s what we use to decide whether our observed F is large enough to be considered statistically significant.

![alt text](image.png)

#### ğŸ”¹ Decision Rule:

* Compute the F-statistic.
* Compare it to the **critical value** from the **F-distribution** (based on chosen significance level and degrees of freedom).
* Or compute the **p-value**:

  * **p < 0.05** â†’ feature significantly affects target (keep it).
  * **p â‰¥ 0.05** â†’ feature likely independent (discard it).

---

#### ğŸ”¹ Python Example:

```python
# Import the required libraries
from sklearn.feature_selection import SelectKBest, f_classif

# Initialize SelectKBest with ANOVA F-test
test = SelectKBest(score_func=f_classif, k=10)

# Fit on your data (X = features, y = categorical target)
X_new = test.fit_transform(X, y)

# Print F-scores for each feature
print(test.scores_)

# Print p-values for significance
print(test.pvalues_)
```

---

#### ğŸ”¹ Pros:

* Very easy and fast to compute.
* Works well for numerical features vs categorical targets.
* Helps quickly rank features before modeling.

#### ğŸ”¹ Cons:

* Assumes data follows a normal distribution.
* Only detects **linear** relationships (can miss non-linear effects).

---

**In short:**

> The ANOVA F-Test compares how much group averages differ (signal) to how much values vary within groups (noise). A high ratio means the feature strongly separates target classes.

### 2.5 Mutual Information (MI)

#### ğŸ”¹ Concept

**Mutual Information (MI)** is a powerful statistical measure from information theory that quantifies **how much information one variable gives about another**. In the context of feature selection, it measures **how much knowing the value of a feature reduces uncertainty about the target variable.**

Unlike correlation, which only measures **linear** relationships, MI can detect **any kind of dependency**â€”linear, non-linear, monotonic, or even complex patterns.

#### ğŸ”¹ Intuition

Imagine youâ€™re trying to predict whether it will rain tomorrow (target variable) using different features:

* If a feature like â€œhumidityâ€ changes in a way that always affects the likelihood of rain, it has **high mutual information** with the target.
* If a feature like â€œshoe sizeâ€ has no connection to rain, it has **zero mutual information**.

In short:

* **High MI â†’** Feature shares a lot of information with the target â†’ very relevant.
* **Low MI â†’** Feature is independent or mostly unrelated â†’ not useful.

---

#### ğŸ”¹ Mathematical Definition

For two variables X (feature) and Y (target), mutual information is defined as:

```math
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)}
```

Where:

* ( P(x, y) ) is the joint probability of X and Y.
* ( P(x) ) and ( P(y) ) are their individual (marginal) probabilities.

The term inside the logarithm measures **how far** the actual joint distribution is from what weâ€™d expect if X and Y were independent.

* If X and Y are independent, ( P(x, y) = P(x)P(y) ), so MI = 0.
* The more X and Y deviate from independence, the larger the MI value.


Letâ€™s think about what the logarithm is doing here.

* When ( P(x, y) > P(x)P(y) ): The pair (x, y) occurs **more often** than expected under independence â†’ the ratio > 1 â†’ log term **positive**.
* When ( P(x, y) < P(x)P(y) ): The pair occurs **less often** than expected â†’ the ratio < 1 â†’ log term **negative**.

Now, since MI multiplies this log term by ( P(x, y) ) (which is always â‰¥ 0) and **sums over all pairs**, the overall effect is that:

* Positive log terms contribute positively.
* Negative log terms contribute less because their associated probabilities ( P(x, y) ) are small (rare events).

When you combine all the weighted contributions, the total always comes out **non-negative**, because MI measures *distance* (in an information sense) between the joint distribution ( P(x, y) ) and the independent distribution ( P(x)P(y) ). Distances cannot be negative.

This idea is deeply connected to **Kullback-Leibler (KL) Divergence**, where MI is essentially:

```math
I(X; Y) = D_{KL}(P(X,Y) \;||\; P(X)P(Y))
```

KL divergence is always â‰¥ 0 because it quantifies how much the real world distribution deviates from independence. So, MI is zero only when X and Y are independent and positive otherwise.

---

#### ğŸ”¹ Intuitive Example for the sin(X) Case

Letâ€™s take the example: X = angle (0Â°â€“360Â°), and Y = sin(X).

* When X = 0Â°, Y = 0.
* When X = 90Â°, Y = 1.
* When X = 270Â°, Y = -1.

Even though the correlation between X and Y is **0**, thereâ€™s still a perfect dependency:

> If you know X, you can exactly determine Y.

Thatâ€™s why MI > 0 â€” it detects that thereâ€™s **a consistent, deterministic link** between X and Y, even though itâ€™s not linear. Correlation misses this because it only looks for a straight-line trend, while MI checks whether knowing one variable gives information about the other, no matter the shape.

---

#### ğŸ’¡ Simplified Intuition

* MI captures *all* types of dependency â€” not just straight lines.
* The log term measures how much each (x, y) pairâ€™s probability deviates from what independence predicts.
* MI adds up all these weighted deviations, always producing a non-negative result.

So, MI can be thought of as the **total reduction in uncertainty about Y** when X is known â€” whether that relationship is linear, curved, oscillating, or anything in between.





#### ğŸ”¹ Intuitive Example

Letâ€™s say you want to predict whether a person buys a product (Yes/No) using age as a feature.

| Age Range | Buy (Yes) | Buy (No) |
| --------- | --------- | -------- |
| 18â€“25     | 80        | 20       |
| 26â€“40     | 60        | 40       |
| 41â€“60     | 20        | 80       |

Here, age and purchase decision clearly depend on each other. Knowing a personâ€™s age group reduces your uncertainty about whether theyâ€™ll buy or not â†’ **high mutual information.**

If instead, the distribution of Yes/No is almost the same across all age groups, then knowing someoneâ€™s age doesnâ€™t help predict the outcome â†’ **low mutual information.**

---

#### ğŸ”¹ Mutual Information vs Correlation

| Aspect                   | Correlation                     | Mutual Information          |
| ------------------------ | ------------------------------- | --------------------------- |
| **Type of relationship** | Linear only                     | Linear + Non-linear         |
| **Scale**                | Between -1 and +1               | Always â‰¥ 0                  |
| **Interpretation**       | Measures direction and strength | Measures shared information |
| **Works with**           | Numeric variables               | Numeric or categorical      |

Thus, MI is more general and robust than correlation, especially in complex datasets where relationships arenâ€™t strictly linear.

---

#### ğŸ”¹ Mutual Information for Feature Selection

In feature selection, we calculate MI between **each feature** and the **target**. Features with higher MI values are more informative and should be kept.

The process:

1. Estimate probabilities ( P(x) ), ( P(y) ), and ( P(x, y) ) from the data.
2. Compute ( I(X; Y) ) for each feature.
3. Rank features by MI value.
4. Select the top features based on a threshold or desired number.

---

#### ğŸ”¹ Python Example

```python
from sklearn.feature_selection import mutual_info_classif, SelectKBest

# mutual_info_classif works for classification targets
selector = SelectKBest(score_func=mutual_info_classif, k=10)
X_new = selector.fit_transform(X, y)

# Display MI scores for each feature
print(selector.scores_)
```

For regression problems, use `mutual_info_regression` instead of `mutual_info_classif`.

---

#### ğŸ”¹ Pros

* Captures both linear and non-linear dependencies.
* Works with both categorical and continuous data.
* Model-independent.

#### ğŸ”¹ Cons

* Estimating probabilities can be tricky for continuous variables.
* Computationally more expensive than simple correlation.

---

#### ğŸ’¡ In Short:

> **Mutual Information measures how much knowing a feature helps predict the target.** The higher the MI, the more useful the feature. Itâ€™s one of the most general and powerful filter methods for feature selection.
